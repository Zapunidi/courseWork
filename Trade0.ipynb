{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    DF_PATH = 'AAPL.csv'\n",
    "    INITIAL_MONEY = 10000\n",
    "    INITIAL_STOCKS = []\n",
    "    OBSERVATION_SPACE = 6\n",
    "    ACTION_SPACE = 3\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv(self.DF_PATH)[:200]\n",
    "        if 'Date' in self.df:\n",
    "            self.df.pop('Date')\n",
    "        self.data = self.df.values\n",
    "        self.current_money = self.INITIAL_MONEY\n",
    "        self.current_stocks = self.INITIAL_STOCKS.copy()\n",
    "        self.cursor = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.cursor = 0\n",
    "        \n",
    "    def sell(self):\n",
    "        if len(self.current_stocks) > 0:\n",
    "            bought = self.current_stocks.pop(0)\n",
    "            sold = self.data[self.cursor][2]\n",
    "            profit = sold - bought\n",
    "        else:\n",
    "            profit = -100\n",
    "        return profit\n",
    "    \n",
    "    def hold(self):\n",
    "        return -1\n",
    "    \n",
    "    def buy(self):\n",
    "        if self.current_money >= self.data[self.cursor][1]:\n",
    "            self.current_money -= self.data[self.cursor][1]\n",
    "            self.current_stocks.append(self.data[self.cursor][1])\n",
    "            self.current_stocks.sort()\n",
    "            return 1\n",
    "        return -100\n",
    "            \n",
    "    def reward(self, profit):\n",
    "        return profit + self.current_money / self.INITIAL_MONEY - self.cursor/len(self.data)\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = self.data[self.cursor]\n",
    "        return state\n",
    "        \n",
    "    def step(self, action):\n",
    "        action -= 1\n",
    "        done = False\n",
    "        self.cursor += 1\n",
    "        \n",
    "        while np.isnan(self.get_state()).any():\n",
    "            self.cursor += 1\n",
    "            print(\"JUST ESCAPED NAN\")\n",
    "        \n",
    "        if action == -1:\n",
    "            profit = self.sell()\n",
    "        elif action == 0:\n",
    "            profit = self.hold()\n",
    "        elif action == 1:\n",
    "            profit = self.buy()\n",
    "            \n",
    "        if self.cursor == len(self.data) - 1:\n",
    "            done = True\n",
    "            \n",
    "        return self.get_state(), self.reward(profit), done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-12-12</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.407747</td>\n",
       "      <td>117258400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-12-15</td>\n",
       "      <td>0.488839</td>\n",
       "      <td>0.488839</td>\n",
       "      <td>0.486607</td>\n",
       "      <td>0.486607</td>\n",
       "      <td>0.386473</td>\n",
       "      <td>43971200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-12-16</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.450893</td>\n",
       "      <td>0.450893</td>\n",
       "      <td>0.358108</td>\n",
       "      <td>26432000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-12-17</td>\n",
       "      <td>0.462054</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.462054</td>\n",
       "      <td>0.462054</td>\n",
       "      <td>0.366972</td>\n",
       "      <td>21610400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-12-18</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>0.477679</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>0.377609</td>\n",
       "      <td>18362400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1988-11-02</td>\n",
       "      <td>1.366071</td>\n",
       "      <td>1.366071</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.330357</td>\n",
       "      <td>1.068099</td>\n",
       "      <td>52130400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1988-11-03</td>\n",
       "      <td>1.330357</td>\n",
       "      <td>1.339286</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.325893</td>\n",
       "      <td>1.064514</td>\n",
       "      <td>60614400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1988-11-04</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.357143</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.348214</td>\n",
       "      <td>1.082435</td>\n",
       "      <td>38449600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1988-11-07</td>\n",
       "      <td>1.330357</td>\n",
       "      <td>1.348214</td>\n",
       "      <td>1.321429</td>\n",
       "      <td>1.339286</td>\n",
       "      <td>1.075267</td>\n",
       "      <td>42520800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1988-11-08</td>\n",
       "      <td>1.339286</td>\n",
       "      <td>1.383929</td>\n",
       "      <td>1.334821</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.103941</td>\n",
       "      <td>38631600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date      Open      High       Low     Close  Adj Close  \\\n",
       "0     1980-12-12  0.513393  0.515625  0.513393  0.513393   0.407747   \n",
       "1     1980-12-15  0.488839  0.488839  0.486607  0.486607   0.386473   \n",
       "2     1980-12-16  0.453125  0.453125  0.450893  0.450893   0.358108   \n",
       "3     1980-12-17  0.462054  0.464286  0.462054  0.462054   0.366972   \n",
       "4     1980-12-18  0.475446  0.477679  0.475446  0.475446   0.377609   \n",
       "...          ...       ...       ...       ...       ...        ...   \n",
       "1995  1988-11-02  1.366071  1.366071  1.312500  1.330357   1.068099   \n",
       "1996  1988-11-03  1.330357  1.339286  1.312500  1.325893   1.064514   \n",
       "1997  1988-11-04  1.312500  1.357143  1.312500  1.348214   1.082435   \n",
       "1998  1988-11-07  1.330357  1.348214  1.321429  1.339286   1.075267   \n",
       "1999  1988-11-08  1.339286  1.383929  1.334821  1.375000   1.103941   \n",
       "\n",
       "           Volume  \n",
       "0     117258400.0  \n",
       "1      43971200.0  \n",
       "2      26432000.0  \n",
       "3      21610400.0  \n",
       "4      18362400.0  \n",
       "...           ...  \n",
       "1995   52130400.0  \n",
       "1996   60614400.0  \n",
       "1997   38449600.0  \n",
       "1998   42520800.0  \n",
       "1999   38631600.0  \n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('AAPL.csv')[:2000]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_106 (Dense)            (None, 24)                168       \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 243\n",
      "Trainable params: 243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_108 (Dense)            (None, 24)                168       \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 1[1. 0. 0.]\n",
      " 2[1. 0. 0.]\n",
      " 3[1. 0. 0.]\n",
      " 4[1. 0. 0.]\n",
      " 5[1. 0. 0.]\n",
      " 6[1. 0. 0.]\n",
      " 7[1. 0. 0.]\n",
      " 8[1. 0. 0.]\n",
      " 9[1. 0. 0.]\n",
      " 10[1. 0. 0.]\n",
      " 11[1. 0. 0.]\n",
      " 12[1. 0. 0.]\n",
      " 13[1. 0. 0.]\n",
      " 14[1. 0. 0.]\n",
      " 15[1. 0. 0.]\n",
      " 16[1. 0. 0.]\n",
      " 17[1. 0. 0.]\n",
      " 18[1. 0. 0.]\n",
      " 19[1. 0. 0.]\n",
      " 20[1. 0. 0.]\n",
      " 21[1. 0. 0.]\n",
      " 22[1. 0. 0.]\n",
      " 23[1. 0. 0.]\n",
      " 24[1. 0. 0.]\n",
      " 25[1. 0. 0.]\n",
      " 26[1. 0. 0.]\n",
      " 27[1. 0. 0.]\n",
      " 28[1. 0. 0.]\n",
      " 29[1. 0. 0.]\n",
      " 30[1. 0. 0.]\n",
      " 31[1. 0. 0.]\n",
      " 32[1. 0. 0.]\n",
      " 33[1. 0. 0.]\n",
      " 34[1. 0. 0.]\n",
      " 35[1. 0. 0.]\n",
      " 36[1. 0. 0.]\n",
      " 37[1. 0. 0.]\n",
      " 38[1. 0. 0.]\n",
      " 39[1. 0. 0.]\n",
      " 40[1. 0. 0.]\n",
      " 41[1. 0. 0.]\n",
      " 42[1. 0. 0.]\n",
      " 43[1. 0. 0.]\n",
      " 44[1. 0. 0.]\n",
      " 45[1. 0. 0.]\n",
      " 46[1. 0. 0.]\n",
      " 47[1. 0. 0.]\n",
      " 48[1. 0. 0.]\n",
      " 49[1. 0. 0.]\n",
      " 50[1. 0. 0.]\n",
      " 51[1. 0. 0.]\n",
      " 52[1. 0. 0.]\n",
      " 53[1. 0. 0.]\n",
      " 54[1. 0. 0.]\n",
      " 55[1. 0. 0.]\n",
      " 56[1. 0. 0.]\n",
      " 57[1. 0. 0.]\n",
      " 58[1. 0. 0.]\n",
      " 59[1. 0. 0.]\n",
      " 60[1. 0. 0.]\n",
      " 61[1. 0. 0.]\n",
      " 62[1. 0. 0.]\n",
      " 63[1. 0. 0.]\n",
      " 64[1. 0. 0.]\n",
      " 65[1. 0. 0.]\n",
      " 66[1. 0. 0.]\n",
      " 67[1. 0. 0.]\n",
      " 68[1. 0. 0.]\n",
      " 69[1. 0. 0.]\n",
      " 70[1. 0. 0.]\n",
      " 71[1. 0. 0.]\n",
      " 72[1. 0. 0.]\n",
      " 73[1. 0. 0.]\n",
      " 74[1. 0. 0.]\n",
      " 75[1. 0. 0.]\n",
      " 76[1. 0. 0.]\n",
      " 77[1. 0. 0.]\n",
      " 78[1. 0. 0.]\n",
      " 79[1. 0. 0.]\n",
      " 80[1. 0. 0.]\n",
      " 81[1. 0. 0.]\n",
      " 82[1. 0. 0.]\n",
      " 83[1. 0. 0.]\n",
      " 84[1. 0. 0.]\n",
      " 85[1. 0. 0.]\n",
      " 86[1. 0. 0.]\n",
      " 87[1. 0. 0.]\n",
      " 88[1. 0. 0.]\n",
      " 89[1. 0. 0.]\n",
      " 90[1. 0. 0.]\n",
      " 91[1. 0. 0.]\n",
      " 92[1. 0. 0.]\n",
      " 93[1. 0. 0.]\n",
      " 94[1. 0. 0.]\n",
      " 95[1. 0. 0.]\n",
      " 96[1. 0. 0.]\n",
      " 97[1. 0. 0.]\n",
      " 98[1. 0. 0.]\n",
      " 99[1. 0. 0.]\n",
      " 100[1. 0. 0.]\n",
      " 101[1. 0. 0.]\n",
      " 102[1. 0. 0.]\n",
      " 103[1. 0. 0.]\n",
      " 104[1. 0. 0.]\n",
      " 105[1. 0. 0.]\n",
      " 106[1. 0. 0.]\n",
      " 107[1. 0. 0.]\n",
      " 108[1. 0. 0.]\n",
      " 109[1. 0. 0.]\n",
      " 110[1. 0. 0.]\n",
      " 111[1. 0. 0.]\n",
      " 112[1. 0. 0.]\n",
      " 113[1. 0. 0.]\n",
      " 114[1. 0. 0.]\n",
      " 115[1. 0. 0.]\n",
      " 116[1. 0. 0.]\n",
      " 117[1. 0. 0.]\n",
      " 118[1. 0. 0.]\n",
      " 119[1. 0. 0.]\n",
      " 120[1. 0. 0.]\n",
      " 121[1. 0. 0.]\n",
      " 122[1. 0. 0.]\n",
      " 123[1. 0. 0.]\n",
      " 124[1. 0. 0.]\n",
      " 125[1. 0. 0.]\n",
      " 126[1. 0. 0.]\n",
      " 127[1. 0. 0.]\n",
      " 128[1. 0. 0.]\n",
      " 129[1. 0. 0.]\n",
      " 130[1. 0. 0.]\n",
      " 131[1. 0. 0.]\n",
      " 132[1. 0. 0.]\n",
      " 133[1. 0. 0.]\n",
      " 134[1. 0. 0.]\n",
      " 135[1. 0. 0.]\n",
      " 136[1. 0. 0.]\n",
      " 137[1. 0. 0.]\n",
      " 138[1. 0. 0.]\n",
      " 139[1. 0. 0.]\n",
      " 140[1. 0. 0.]\n",
      " 141[1. 0. 0.]\n",
      " 142[1. 0. 0.]\n",
      " 143[1. 0. 0.]\n",
      " 144[1. 0. 0.]\n",
      " 145[1. 0. 0.]\n",
      " 146[1. 0. 0.]\n",
      " 147[1. 0. 0.]\n",
      " 148[1. 0. 0.]\n",
      " 149[1. 0. 0.]\n",
      " 150[1. 0. 0.]\n",
      " 151[1. 0. 0.]\n",
      " 152[1. 0. 0.]\n",
      " 153[1. 0. 0.]\n",
      " 154[1. 0. 0.]\n",
      " 155[1. 0. 0.]\n",
      " 156[1. 0. 0.]\n",
      " 157[1. 0. 0.]\n",
      " 158[1. 0. 0.]\n",
      " 159[1. 0. 0.]\n",
      " 160[1. 0. 0.]\n",
      " 161[1. 0. 0.]\n",
      " 162[1. 0. 0.]\n",
      " 163[1. 0. 0.]\n",
      " 164[1. 0. 0.]\n",
      " 165[1. 0. 0.]\n",
      "JUST ESCAPED NAN\n",
      " 166[1. 0. 0.]\n",
      " 167[1. 0. 0.]\n",
      " 168[1. 0. 0.]\n",
      " 169[1. 0. 0.]\n",
      " 170[1. 0. 0.]\n",
      " 171[1. 0. 0.]\n",
      " 172[1. 0. 0.]\n",
      " 173[1. 0. 0.]\n",
      " 174[1. 0. 0.]\n",
      " 175[1. 0. 0.]\n",
      " 176[1. 0. 0.]\n",
      " 177[1. 0. 0.]\n",
      " 178[1. 0. 0.]\n",
      " 179[1. 0. 0.]\n",
      " 180[1. 0. 0.]\n",
      " 181[1. 0. 0.]\n",
      " 182[1. 0. 0.]\n",
      " 183[1. 0. 0.]\n",
      " 184[1. 0. 0.]\n",
      " 185[1. 0. 0.]\n",
      " 186[1. 0. 0.]\n",
      " 187[1. 0. 0.]\n",
      " 188[1. 0. 0.]\n",
      " 189[1. 0. 0.]\n",
      " 190[1. 0. 0.]\n",
      " 191[1. 0. 0.]\n",
      " 192[1. 0. 0.]\n",
      " 193[1. 0. 0.]\n",
      " 194[1. 0. 0.]\n",
      " 195[1. 0. 0.]\n",
      " 196[1. 0. 0.]\n",
      " 197[1. 0. 0.]\n",
      " 198[1. 0. 0.]\n",
      "episode: 0   score: -19700.675\n",
      " 1[1. 0. 0.]\n",
      " 2[1. 0. 0.]\n",
      " 3[1. 0. 0.]\n",
      " 4[1. 0. 0.]\n",
      " 5[1. 0. 0.]\n",
      " 6[1. 0. 0.]\n",
      " 7[1. 0. 0.]\n",
      " 8[1. 0. 0.]\n",
      " 9[1. 0. 0.]\n",
      " 10[1. 0. 0.]\n",
      " 11[1. 0. 0.]\n",
      " 12[1. 0. 0.]\n",
      " 13[1. 0. 0.]\n",
      " 14[1. 0. 0.]\n",
      " 15[1. 0. 0.]\n",
      " 16[1. 0. 0.]\n",
      " 17[1. 0. 0.]\n",
      " 18[1. 0. 0.]\n",
      " 19[1. 0. 0.]\n",
      " 20[1. 0. 0.]\n",
      " 21[1. 0. 0.]\n",
      " 22[1. 0. 0.]\n",
      " 23[1. 0. 0.]\n",
      " 24[1. 0. 0.]\n",
      " 25[1. 0. 0.]\n",
      " 26[1. 0. 0.]\n",
      " 27[1. 0. 0.]\n",
      " 28[1. 0. 0.]\n",
      " 29[1. 0. 0.]\n",
      " 30[1. 0. 0.]\n",
      " 31[1. 0. 0.]\n",
      " 32[1. 0. 0.]\n",
      " 33[1. 0. 0.]\n",
      " 34[1. 0. 0.]\n",
      " 35[1. 0. 0.]\n",
      " 36[1. 0. 0.]\n",
      " 37[1. 0. 0.]\n",
      " 38[1. 0. 0.]\n",
      " 39[1. 0. 0.]\n",
      " 40[1. 0. 0.]\n",
      " 41[1. 0. 0.]\n",
      " 42[1. 0. 0.]\n",
      " 43[1. 0. 0.]\n",
      " 44[1. 0. 0.]\n",
      " 45[1. 0. 0.]\n",
      " 46[1. 0. 0.]\n",
      " 47[1. 0. 0.]\n",
      " 48[1. 0. 0.]\n",
      " 49[1. 0. 0.]\n",
      " 50[1. 0. 0.]\n",
      " 51[1. 0. 0.]\n",
      " 52[1. 0. 0.]\n",
      " 53[1. 0. 0.]\n",
      " 54[1. 0. 0.]\n",
      " 55[1. 0. 0.]\n",
      " 56[1. 0. 0.]\n",
      " 57[1. 0. 0.]\n",
      " 58[1. 0. 0.]\n",
      " 59[1. 0. 0.]\n",
      " 60[1. 0. 0.]\n",
      " 61[1. 0. 0.]\n",
      " 62[1. 0. 0.]\n",
      " 63[1. 0. 0.]\n",
      " 64[1. 0. 0.]\n",
      " 65[1. 0. 0.]\n",
      " 66[1. 0. 0.]\n",
      " 67[1. 0. 0.]\n",
      " 68[1. 0. 0.]\n",
      " 69[1. 0. 0.]\n",
      " 70[1. 0. 0.]\n",
      " 71[1. 0. 0.]\n",
      " 72[1. 0. 0.]\n",
      " 73[1. 0. 0.]\n",
      " 74[1. 0. 0.]\n",
      " 75[1. 0. 0.]\n",
      " 76[1. 0. 0.]\n",
      " 77[1. 0. 0.]\n",
      " 78[1. 0. 0.]\n",
      " 79[1. 0. 0.]\n",
      " 80[1. 0. 0.]\n",
      " 81[1. 0. 0.]\n",
      " 82[1. 0. 0.]\n",
      " 83[1. 0. 0.]\n",
      " 84[1. 0. 0.]\n",
      " 85[1. 0. 0.]\n",
      " 86[1. 0. 0.]\n",
      " 87[1. 0. 0.]\n",
      " 88[1. 0. 0.]\n",
      " 89[1. 0. 0.]\n",
      " 90[1. 0. 0.]\n",
      " 91[1. 0. 0.]\n",
      " 92[1. 0. 0.]\n",
      " 93[1. 0. 0.]\n",
      " 94[1. 0. 0.]\n",
      " 95[1. 0. 0.]\n",
      " 96[1. 0. 0.]\n",
      " 97[1. 0. 0.]\n",
      " 98[1. 0. 0.]\n",
      " 99[1. 0. 0.]\n",
      " 100[1. 0. 0.]\n",
      " 101[1. 0. 0.]\n",
      " 102[1. 0. 0.]\n",
      " 103[1. 0. 0.]\n",
      " 104[1. 0. 0.]\n",
      " 105[1. 0. 0.]\n",
      " 106[1. 0. 0.]\n",
      " 107[1. 0. 0.]\n",
      " 108[1. 0. 0.]\n",
      " 109[1. 0. 0.]\n",
      " 110[1. 0. 0.]\n",
      " 111[1. 0. 0.]\n",
      " 112[1. 0. 0.]\n",
      " 113[1. 0. 0.]\n",
      " 114[1. 0. 0.]\n",
      " 115[1. 0. 0.]\n",
      " 116[1. 0. 0.]\n",
      " 117[1. 0. 0.]\n",
      " 118[1. 0. 0.]\n",
      " 119[1. 0. 0.]\n",
      " 120[1. 0. 0.]\n",
      " 121[1. 0. 0.]\n",
      " 122[1. 0. 0.]\n",
      " 123[1. 0. 0.]\n",
      " 124[1. 0. 0.]\n",
      " 125[1. 0. 0.]\n",
      " 126[1. 0. 0.]\n",
      " 127[1. 0. 0.]\n",
      " 128[1. 0. 0.]\n",
      " 129[1. 0. 0.]\n",
      " 130[1. 0. 0.]\n",
      " 131[1. 0. 0.]\n",
      " 132[1. 0. 0.]\n",
      " 133[1. 0. 0.]\n",
      " 134[1. 0. 0.]\n",
      " 135[1. 0. 0.]\n",
      " 136[1. 0. 0.]\n",
      " 137[1. 0. 0.]\n",
      " 138[1. 0. 0.]\n",
      " 139[1. 0. 0.]\n",
      " 140[1. 0. 0.]\n",
      " 141[1. 0. 0.]\n",
      " 142[1. 0. 0.]\n",
      " 143[1. 0. 0.]\n",
      " 144[1. 0. 0.]\n",
      " 145[1. 0. 0.]\n",
      " 146[1. 0. 0.]\n",
      " 147[1. 0. 0.]\n",
      " 148[1. 0. 0.]\n",
      " 149[1. 0. 0.]\n",
      " 150[1. 0. 0.]\n",
      " 151[1. 0. 0.]\n",
      " 152[1. 0. 0.]\n",
      " 153[1. 0. 0.]\n",
      " 154[1. 0. 0.]\n",
      " 155[1. 0. 0.]\n",
      " 156[1. 0. 0.]\n",
      " 157[1. 0. 0.]\n",
      " 158[1. 0. 0.]\n",
      " 159[1. 0. 0.]\n",
      " 160[1. 0. 0.]\n",
      " 161[1. 0. 0.]\n",
      " 162[1. 0. 0.]\n",
      " 163[1. 0. 0.]\n",
      " 164[1. 0. 0.]\n",
      " 165[1. 0. 0.]\n",
      "JUST ESCAPED NAN\n",
      " 166[1. 0. 0.]\n",
      " 167[1. 0. 0.]\n",
      " 168[1. 0. 0.]\n",
      " 169[1. 0. 0.]\n",
      " 170[1. 0. 0.]\n",
      " 171[1. 0. 0.]\n",
      " 172[1. 0. 0.]\n",
      " 173[1. 0. 0.]\n",
      " 174[1. 0. 0.]\n",
      " 175[1. 0. 0.]\n",
      " 176[1. 0. 0.]\n",
      " 177[1. 0. 0.]\n",
      " 178[1. 0. 0.]\n",
      " 179[1. 0. 0.]\n",
      " 180[1. 0. 0.]\n",
      " 181[1. 0. 0.]\n",
      " 182[1. 0. 0.]\n",
      " 183[1. 0. 0.]\n",
      " 184[1. 0. 0.]\n",
      " 185[1. 0. 0.]\n",
      " 186[1. 0. 0.]\n",
      " 187[1. 0. 0.]\n",
      " 188[1. 0. 0.]\n",
      " 189[1. 0. 0.]\n",
      " 190[1. 0. 0.]\n",
      " 191[1. 0. 0.]\n",
      " 192[1. 0. 0.]\n",
      " 193[1. 0. 0.]\n",
      " 194[1. 0. 0.]\n",
      " 195[1. 0. 0.]\n",
      " 196[1. 0. 0.]\n",
      " 197[1. 0. 0.]\n",
      " 198[1. 0. 0.]\n",
      "episode: 1   score: -19700.675\n",
      " 1[1. 0. 0.]\n",
      " 2[1. 0. 0.]\n",
      " 3[1. 0. 0.]\n",
      " 4[1. 0. 0.]\n",
      " 5[1. 0. 0.]\n",
      " 6[1. 0. 0.]\n",
      " 7[1. 0. 0.]\n",
      " 8[1. 0. 0.]\n",
      " 9[1. 0. 0.]\n",
      " 10[1. 0. 0.]\n",
      " 11[1. 0. 0.]\n",
      " 12[1. 0. 0.]\n",
      " 13[1. 0. 0.]\n",
      " 14[1. 0. 0.]\n",
      " 15[1. 0. 0.]\n",
      " 16[1. 0. 0.]\n",
      " 17[1. 0. 0.]\n",
      " 18[1. 0. 0.]\n",
      " 19[1. 0. 0.]\n",
      " 20[1. 0. 0.]\n",
      " 21[1. 0. 0.]\n",
      " 22[1. 0. 0.]\n",
      " 23[1. 0. 0.]\n",
      " 24[1. 0. 0.]\n",
      " 25[1. 0. 0.]\n",
      " 26[1. 0. 0.]\n",
      " 27[1. 0. 0.]\n",
      " 28[1. 0. 0.]\n",
      " 29[1. 0. 0.]\n",
      " 30[1. 0. 0.]\n",
      " 31[1. 0. 0.]\n",
      " 32[1. 0. 0.]\n",
      " 33[1. 0. 0.]\n",
      " 34[1. 0. 0.]\n",
      " 35[1. 0. 0.]\n",
      " 36[1. 0. 0.]\n",
      " 37[1. 0. 0.]\n",
      " 38[1. 0. 0.]\n",
      " 39[1. 0. 0.]\n",
      " 40[1. 0. 0.]\n",
      " 41[1. 0. 0.]\n",
      " 42[1. 0. 0.]\n",
      " 43[1. 0. 0.]\n",
      " 44[1. 0. 0.]\n",
      " 45[1. 0. 0.]\n",
      " 46[1. 0. 0.]\n",
      " 47[1. 0. 0.]\n",
      " 48[1. 0. 0.]\n",
      " 49[1. 0. 0.]\n",
      " 50[1. 0. 0.]\n",
      " 51[1. 0. 0.]\n",
      " 52[1. 0. 0.]\n",
      " 53[1. 0. 0.]\n",
      " 54[1. 0. 0.]\n",
      " 55[1. 0. 0.]\n",
      " 56[1. 0. 0.]\n",
      " 57[1. 0. 0.]\n",
      " 58[1. 0. 0.]\n",
      " 59[1. 0. 0.]\n",
      " 60[1. 0. 0.]\n",
      " 61[1. 0. 0.]\n",
      " 62[1. 0. 0.]\n",
      " 63[1. 0. 0.]\n",
      " 64[1. 0. 0.]\n",
      " 65[1. 0. 0.]\n",
      " 66[1. 0. 0.]\n",
      " 67[1. 0. 0.]\n",
      " 68[1. 0. 0.]\n",
      " 69[1. 0. 0.]\n",
      " 70[1. 0. 0.]\n",
      " 71[1. 0. 0.]\n",
      " 72[1. 0. 0.]\n",
      " 73[1. 0. 0.]\n",
      " 74[1. 0. 0.]\n",
      " 75[1. 0. 0.]\n",
      " 76[1. 0. 0.]\n",
      " 77[1. 0. 0.]\n",
      " 78[1. 0. 0.]\n",
      " 79[1. 0. 0.]\n",
      " 80[1. 0. 0.]\n",
      " 81[1. 0. 0.]\n",
      " 82[1. 0. 0.]\n",
      " 83[1. 0. 0.]\n",
      " 84[1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85[1. 0. 0.]\n",
      " 86[1. 0. 0.]\n",
      " 87[1. 0. 0.]\n",
      " 88[1. 0. 0.]\n",
      " 89[1. 0. 0.]\n",
      " 90[1. 0. 0.]\n",
      " 91[1. 0. 0.]\n",
      " 92[1. 0. 0.]\n",
      " 93[1. 0. 0.]\n",
      " 94[1. 0. 0.]\n",
      " 95[1. 0. 0.]\n",
      " 96[1. 0. 0.]\n",
      " 97[1. 0. 0.]\n",
      " 98[1. 0. 0.]\n",
      " 99[1. 0. 0.]\n",
      " 100[1. 0. 0.]\n",
      " 101[1. 0. 0.]\n",
      " 102[1. 0. 0.]\n",
      " 103[1. 0. 0.]\n",
      " 104[1. 0. 0.]\n",
      " 105[1. 0. 0.]\n",
      " 106[1. 0. 0.]\n",
      " 107[1. 0. 0.]\n",
      " 108[1. 0. 0.]\n",
      " 109[1. 0. 0.]\n",
      " 110[1. 0. 0.]\n",
      " 111[1. 0. 0.]\n",
      " 112[1. 0. 0.]\n",
      " 113[1. 0. 0.]\n",
      " 114[1. 0. 0.]\n",
      " 115[1. 0. 0.]\n",
      " 116[1. 0. 0.]\n",
      " 117[1. 0. 0.]\n",
      " 118[1. 0. 0.]\n",
      " 119[1. 0. 0.]\n",
      " 120[1. 0. 0.]\n",
      " 121[1. 0. 0.]\n",
      " 122[1. 0. 0.]\n",
      " 123[1. 0. 0.]\n",
      " 124[1. 0. 0.]\n",
      " 125[1. 0. 0.]\n",
      " 126[1. 0. 0.]\n",
      " 127[1. 0. 0.]\n",
      " 128[1. 0. 0.]\n",
      " 129[1. 0. 0.]\n",
      " 130[1. 0. 0.]\n",
      " 131[1. 0. 0.]\n",
      " 132[1. 0. 0.]\n",
      " 133[1. 0. 0.]\n",
      " 134[1. 0. 0.]\n",
      " 135[1. 0. 0.]\n",
      " 136[1. 0. 0.]\n",
      " 137[1. 0. 0.]\n",
      " 138[1. 0. 0.]\n",
      " 139[1. 0. 0.]\n",
      " 140[1. 0. 0.]\n",
      " 141[1. 0. 0.]\n",
      " 142[1. 0. 0.]\n",
      " 143[1. 0. 0.]\n",
      " 144[1. 0. 0.]\n",
      " 145[1. 0. 0.]\n",
      " 146[1. 0. 0.]\n",
      " 147[1. 0. 0.]\n",
      " 148[1. 0. 0.]\n",
      " 149[1. 0. 0.]\n",
      " 150[1. 0. 0.]\n",
      " 151[1. 0. 0.]\n",
      " 152[1. 0. 0.]\n",
      " 153[1. 0. 0.]\n",
      " 154[1. 0. 0.]\n",
      " 155[1. 0. 0.]\n",
      " 156[1. 0. 0.]\n",
      " 157[1. 0. 0.]\n",
      " 158[1. 0. 0.]\n",
      " 159[1. 0. 0.]\n",
      " 160[1. 0. 0.]\n",
      " 161[1. 0. 0.]\n",
      " 162[1. 0. 0.]\n",
      " 163[1. 0. 0.]\n",
      " 164[1. 0. 0.]\n",
      " 165[1. 0. 0.]\n",
      "JUST ESCAPED NAN\n",
      " 166[1. 0. 0.]\n",
      " 167[1. 0. 0.]\n",
      " 168[1. 0. 0.]\n",
      " 169[1. 0. 0.]\n",
      " 170[1. 0. 0.]\n",
      " 171[1. 0. 0.]\n",
      " 172[1. 0. 0.]\n",
      " 173[1. 0. 0.]\n",
      " 174[1. 0. 0.]\n",
      " 175[1. 0. 0.]\n",
      " 176[1. 0. 0.]\n",
      " 177[1. 0. 0.]\n",
      " 178[1. 0. 0.]\n",
      " 179[1. 0. 0.]\n",
      " 180[1. 0. 0.]\n",
      " 181[1. 0. 0.]\n",
      " 182[1. 0. 0.]\n",
      " 183[1. 0. 0.]\n",
      " 184[1. 0. 0.]\n",
      " 185[1. 0. 0.]\n",
      " 186[1. 0. 0.]\n",
      " 187[1. 0. 0.]\n",
      " 188[1. 0. 0.]\n",
      " 189[1. 0. 0.]\n",
      " 190[1. 0. 0.]\n",
      " 191[1. 0. 0.]\n",
      " 192[1. 0. 0.]\n",
      " 193[1. 0. 0.]\n",
      " 194[1. 0. 0.]\n",
      " 195[1. 0. 0.]\n",
      " 196[1. 0. 0.]\n",
      " 197[1. 0. 0.]\n",
      " 198[1. 0. 0.]\n",
      "episode: 2   score: -19700.675\n",
      " 1[1. 0. 0.]\n",
      " 2[1. 0. 0.]\n",
      " 3[1. 0. 0.]\n",
      " 4[1. 0. 0.]\n",
      " 5[1. 0. 0.]\n",
      " 6[1. 0. 0.]\n",
      " 7[1. 0. 0.]\n",
      " 8[1. 0. 0.]\n",
      " 9[1. 0. 0.]\n",
      " 10[1. 0. 0.]\n",
      " 11[1. 0. 0.]\n",
      " 12[1. 0. 0.]\n",
      " 13[1. 0. 0.]\n",
      " 14[1. 0. 0.]\n",
      " 15[1. 0. 0.]\n",
      " 16[1. 0. 0.]\n",
      " 17[1. 0. 0.]\n",
      " 18[1. 0. 0.]\n",
      " 19[1. 0. 0.]\n",
      " 20[1. 0. 0.]\n",
      " 21[1. 0. 0.]\n",
      " 22[1. 0. 0.]\n",
      " 23[1. 0. 0.]\n",
      " 24[1. 0. 0.]\n",
      " 25[1. 0. 0.]\n",
      " 26[1. 0. 0.]\n",
      " 27[1. 0. 0.]\n",
      " 28[1. 0. 0.]\n",
      " 29[1. 0. 0.]\n",
      " 30[1. 0. 0.]\n",
      " 31[1. 0. 0.]\n",
      " 32[1. 0. 0.]\n",
      " 33[1. 0. 0.]\n",
      " 34[1. 0. 0.]\n",
      " 35[1. 0. 0.]\n",
      " 36[1. 0. 0.]\n",
      " 37[1. 0. 0.]\n",
      " 38[1. 0. 0.]\n",
      " 39[1. 0. 0.]\n",
      " 40[1. 0. 0.]\n",
      " 41[1. 0. 0.]\n",
      " 42[1. 0. 0.]\n",
      " 43[1. 0. 0.]\n",
      " 44[1. 0. 0.]\n",
      " 45[1. 0. 0.]\n",
      " 46[1. 0. 0.]\n",
      " 47[1. 0. 0.]\n",
      " 48[1. 0. 0.]\n",
      " 49[1. 0. 0.]\n",
      " 50[1. 0. 0.]\n",
      " 51[1. 0. 0.]\n",
      " 52[1. 0. 0.]\n",
      " 53[1. 0. 0.]\n",
      " 54[1. 0. 0.]\n",
      " 55[1. 0. 0.]\n",
      " 56[1. 0. 0.]\n",
      " 57[1. 0. 0.]\n",
      " 58[1. 0. 0.]\n",
      " 59[1. 0. 0.]\n",
      " 60[1. 0. 0.]\n",
      " 61[1. 0. 0.]\n",
      " 62[1. 0. 0.]\n",
      " 63[1. 0. 0.]\n",
      " 64[1. 0. 0.]\n",
      " 65[1. 0. 0.]\n",
      " 66[1. 0. 0.]\n",
      " 67[1. 0. 0.]\n",
      " 68[1. 0. 0.]\n",
      " 69[1. 0. 0.]\n",
      " 70[1. 0. 0.]\n",
      " 71[1. 0. 0.]\n",
      " 72[1. 0. 0.]\n",
      " 73[1. 0. 0.]\n",
      " 74[1. 0. 0.]\n",
      " 75[1. 0. 0.]\n",
      " 76[1. 0. 0.]\n",
      " 77[1. 0. 0.]\n",
      " 78[1. 0. 0.]\n",
      " 79[1. 0. 0.]\n",
      " 80[1. 0. 0.]\n",
      " 81[1. 0. 0.]\n",
      " 82[1. 0. 0.]\n",
      " 83[1. 0. 0.]\n",
      " 84[1. 0. 0.]\n",
      " 85[1. 0. 0.]\n",
      " 86[1. 0. 0.]\n",
      " 87[1. 0. 0.]\n",
      " 88[1. 0. 0.]\n",
      " 89[1. 0. 0.]\n",
      " 90[1. 0. 0.]\n",
      " 91[1. 0. 0.]\n",
      " 92[1. 0. 0.]\n",
      " 93[1. 0. 0.]\n",
      " 94[1. 0. 0.]\n",
      " 95[1. 0. 0.]\n",
      " 96[1. 0. 0.]\n",
      " 97[1. 0. 0.]\n",
      " 98[1. 0. 0.]\n",
      " 99[1. 0. 0.]\n",
      " 100[1. 0. 0.]\n",
      " 101[1. 0. 0.]\n",
      " 102[1. 0. 0.]\n",
      " 103[1. 0. 0.]\n",
      " 104[1. 0. 0.]\n",
      " 105[1. 0. 0.]\n",
      " 106[1. 0. 0.]\n",
      " 107[1. 0. 0.]\n",
      " 108[1. 0. 0.]\n",
      " 109[1. 0. 0.]\n",
      " 110[1. 0. 0.]\n",
      " 111[1. 0. 0.]\n",
      " 112[1. 0. 0.]\n",
      " 113[1. 0. 0.]\n",
      " 114[1. 0. 0.]\n",
      " 115[1. 0. 0.]\n",
      " 116[1. 0. 0.]\n",
      " 117[1. 0. 0.]\n",
      " 118[1. 0. 0.]\n",
      " 119[1. 0. 0.]\n",
      " 120[1. 0. 0.]\n",
      " 121[1. 0. 0.]\n",
      " 122[1. 0. 0.]\n",
      " 123[1. 0. 0.]\n",
      " 124[1. 0. 0.]\n",
      " 125[1. 0. 0.]\n",
      " 126[1. 0. 0.]\n",
      " 127[1. 0. 0.]\n",
      " 128[1. 0. 0.]\n",
      " 129[1. 0. 0.]\n",
      " 130[1. 0. 0.]\n",
      " 131[1. 0. 0.]\n",
      " 132[1. 0. 0.]\n",
      " 133[1. 0. 0.]\n",
      " 134[1. 0. 0.]\n",
      " 135[1. 0. 0.]\n",
      " 136[1. 0. 0.]\n",
      " 137[1. 0. 0.]\n",
      " 138[1. 0. 0.]\n",
      " 139[1. 0. 0.]\n",
      " 140[1. 0. 0.]\n",
      " 141[1. 0. 0.]\n",
      " 142[1. 0. 0.]\n",
      " 143[1. 0. 0.]\n",
      " 144[1. 0. 0.]\n",
      " 145[1. 0. 0.]\n",
      " 146[1. 0. 0.]\n",
      " 147[1. 0. 0.]\n",
      " 148[1. 0. 0.]\n",
      " 149[1. 0. 0.]\n",
      " 150[1. 0. 0.]\n",
      " 151[1. 0. 0.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-516928cf8817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-516928cf8817>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import pylab\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EPISODES = 10\n",
    "\n",
    "\n",
    "# A2C(Advantage Actor-Critic) agent for the Cartpole\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.0001\n",
    "        self.critic_lr = 0.0005\n",
    "\n",
    "        # create model for policy network\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        #if self.load_model:\n",
    "        #    self.actor.load_weights(\"./save_model/cartpole_actor.h5\")\n",
    "        #    self.critic.load_weights(\"./save_model/cartpole_critic.h5\")\n",
    "\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor: state is input and probability of each action is output of model\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        #actor.add(Dense(12, activation='relu',\n",
    "        #                kernel_initializer='he_uniform')),\n",
    "        actor.add(Dense(self.action_size, activation='softmax',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        actor.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        #critic.add(Dense(12, activation='relu',\n",
    "        #                 kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(self.value_size, activation='linear',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "        print(policy)\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        self.critic.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    env = Environment()\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.OBSERVATION_SPACE\n",
    "    action_size = env.ACTION_SPACE\n",
    "\n",
    "    # make A2C agent\n",
    "    agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        env.reset()\n",
    "        state = env.get_state()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        c = 0\n",
    "        while not done:\n",
    "            c += 1\n",
    "            print('\\r', c, end='')\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode, plot the play time\n",
    "                #score = score if score == 500.0 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"cartpole_a2c.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                #if np.mean(scores[-min(10, len(scores)):]) > -10:\n",
    "                #    break\n",
    "\n",
    "        # save the model\n",
    "        #if e % 25 == 0:\n",
    "        #    agent.actor.save_weights(\"./save_model/cartpole_actor2.h5\")\n",
    "        #    agent.critic.save_weights(\"./save_model/cartpole_critic2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1981-08-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1981-08-11</td>\n",
       "      <td>0.441964</td>\n",
       "      <td>0.441964</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.347471</td>\n",
       "      <td>17864000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      Open      High     Low   Close  Adj Close      Volume\n",
       "165  1981-08-10       NaN       NaN     NaN     NaN        NaN         NaN\n",
       "166  1981-08-11  0.441964  0.441964  0.4375  0.4375   0.347471  17864000.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[165:167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
